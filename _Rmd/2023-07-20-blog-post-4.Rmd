---
title: "Blog Post 4"
author: "Sandra Aziz"
output: "github_document"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## My Favorite Machine Learning Method

The machine learning method that I found the most interesting is the Random Forests method. It is an ensemble learning method that combines multiple decision trees to improve prediction performance and reduce over-fitting. It works by building multiple decision trees on random subsets of the training data then aggregating their predictions to make a final decision. An advantage of this method is that it is able to handle both categorical and numerical data.  

Below is an example of a random forest model:  

```{r iris, message=FALSE}
# Load required libraries
library(randomForest)

# Load the Iris dataset
data(iris)

# Split the data into training and testing sets
set.seed(123) # For reproducibility
train_indices <- sample(1:nrow(iris), 0.7 * nrow(iris))
train_data <- iris[train_indices, ]
test_data <- iris[-train_indices, ]

# Fit the Random Forest model
rf_model <- randomForest(Species ~ ., data = train_data, ntree = 100)

# Make predictions on the test set
predictions <- predict(rf_model, test_data)

# Evaluate the model performance
conf_matrix <- table(predictions, test_data$Species)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)

# Print the confusion matrix and accuracy
print("Confusion Matrix:")
print(conf_matrix)
print(paste("Accuracy:", accuracy))
```


```{r render, echo=FALSE, eval=FALSE}
rmarkdown::render("~/Library/CloudStorage/OneDrive-NorthCarolinaStateUniversity/NCSU/Summer2023/ST 558/saziz12.github.io/_Rmd/2023-07-20-blog-post-4.Rmd",
output_format = "github_document", 
output_dir = "_posts/",
output_options = list(html_preview=FALSE))
```
