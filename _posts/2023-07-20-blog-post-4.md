Blog Post 4
================
Sandra Aziz

## My Favorite Machine Learning Method

The machine learning method that I found the most interesting is the
Random Forests method. It is an ensemble learning method that combines
multiple decision trees to improve prediction performance and reduce
over-fitting. It works by building multiple decision trees on random
subsets of the training data then aggregating their predictions to make
a final decision. An advantage of this method is that it is able to
handle both categorical and numerical data.

Below is an example of a random forest model:

``` r
# Load required libraries
library(randomForest)

# Load the Iris dataset
data(iris)

# Split the data into training and testing sets
set.seed(123) # For reproducibility
train_indices <- sample(1:nrow(iris), 0.7 * nrow(iris))
train_data <- iris[train_indices, ]
test_data <- iris[-train_indices, ]

# Fit the Random Forest model
rf_model <- randomForest(Species ~ ., data = train_data, ntree = 100)

# Make predictions on the test set
predictions <- predict(rf_model, test_data)

# Evaluate the model performance
conf_matrix <- table(predictions, test_data$Species)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)

# Print the confusion matrix and accuracy
print("Confusion Matrix:")
```

    ## [1] "Confusion Matrix:"

``` r
print(conf_matrix)
```

    ##             
    ## predictions  setosa versicolor virginica
    ##   setosa         14          0         0
    ##   versicolor      0         17         0
    ##   virginica       0          1        13

``` r
print(paste("Accuracy:", accuracy))
```

    ## [1] "Accuracy: 0.977777777777778"
